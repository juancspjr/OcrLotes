"""
Rutas y controladores para la aplicaci√≥n web Flask
"""

import os
import json
import logging
import shutil
import psutil
import uuid
import time
from pathlib import Path
from datetime import datetime
from werkzeug.utils import secure_filename
from flask import render_template, request, redirect, url_for, flash, jsonify, send_file

from app import app, preload_ocr_components, start_batch_worker
import app as app_module
from main_ocr_process import OrquestadorOCR
import config

# FIX: Logger configurado correctamente para routes.py
# REASON: Variable logger usada 57 veces pero no estaba definida causando NameError
# IMPACT: Eliminaci√≥n completa de errores de logging en producci√≥n
# TEST: Verificar que todos los logger.info/debug/warning/error funcionen
# MONITOR: Logging enterprise configurado con nivel y formato apropiado
# INTERFACE: Logging coherente en toda la aplicaci√≥n
# VISUAL_CHANGE: Logs visibles y estructurados en consola y archivos
# REFERENCE_INTEGRITY: Variable logger ahora existe y est√° configurada correctamente
logger = logging.getLogger(__name__)

logger = logging.getLogger(__name__)

def validate_whatsapp_metadata(metadata_dict):
    """
    FIX: Validaci√≥n robusta de metadatos WhatsApp siguiendo filosof√≠a Zero-Fault Detection
    REASON: Usuario requiere validaci√≥n estricta de formato de metadatos WhatsApp
    IMPACT: Garantiza integridad de datos y previene errores en procesamiento
    INTERFACE: Validaci√≥n enterprise con reportes detallados de errores
    """
    errors = []
    warnings = []
    
    # Validar numerosorteo: debe ser A-Z o 01-99
    numerosorteo = metadata_dict.get('numerosorteo', '').strip()
    if numerosorteo:
        if not (numerosorteo.isalpha() and len(numerosorteo) == 1 and numerosorteo.isupper()) and \
           not (numerosorteo.isdigit() and 1 <= int(numerosorteo) <= 99):
            errors.append(f"numerosorteo '{numerosorteo}' debe ser A-Z o 01-99")
    
    # Validar fechasorteo: formato YYYYMMDD
    fechasorteo = metadata_dict.get('fechasorteo', '').strip()
    if fechasorteo and len(fechasorteo) != 8 or not fechasorteo.isdigit():
        errors.append(f"fechasorteo '{fechasorteo}' debe ser formato YYYYMMDD")
    
    # Validar idWhatsapp: debe terminar en @lid
    idWhatsapp = metadata_dict.get('idWhatsapp', '').strip()
    if idWhatsapp and not idWhatsapp.endswith('@lid'):
        warnings.append(f"idWhatsapp '{idWhatsapp}' deber√≠a terminar en @lid")
    
    # Validar horamin: formato HH-MM
    horamin = metadata_dict.get('horamin', '').strip()
    if horamin:
        if len(horamin) != 5 or horamin[2] != '-' or not horamin[:2].isdigit() or not horamin[3:].isdigit():
            errors.append(f"horamin '{horamin}' debe ser formato HH-MM")
        else:
            hora, minuto = horamin.split('-')
            if not (0 <= int(hora) <= 23) or not (0 <= int(minuto) <= 59):
                errors.append(f"horamin '{horamin}' tiene valores fuera de rango")
    
    # Validar nombre: no debe estar vac√≠o si se proporciona
    nombre = metadata_dict.get('nombre', '').strip()
    if nombre and len(nombre) < 2:
        warnings.append(f"nombre '{nombre}' es muy corto")
    
    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'warnings': warnings
    }

# Variables globales para estado del sistema as√≠ncrono
_worker_running = False
_worker_thread = None

# Inicializaci√≥n del sistema
try:
    logger.info("Inicializando sistema OCR as√≠ncrono...")
    
    # Pre-cargar componentes OCR
    preload_ocr_components()
    
    # Inicializar worker as√≠ncrono
    start_batch_worker()
    
    logger.info("‚úÖ Sistema OCR as√≠ncrono inicializado exitosamente")
    
    # Inicializar directorios y configuraciones
    from config import get_async_directories, get_api_config
    directories = get_async_directories()
    api_config = get_api_config()
    
    logger.info("‚úÖ Rutas API HTTP y directorios as√≠ncronos inicializados")
    
except Exception as e:
    logger.error(f"Error inicializando sistema OCR: {e}")

@app.route('/')
def index():
    """
    FIX: P√°gina principal con Interface Excellence Dashboard
    REASON: Following Interface Excellence philosophy for enhanced filename visibility and professional appearance
    IMPACT: Better user experience with enhanced file tracking and external environment styling
    INTERFACE: Comprehensive validation and enhanced UI components with real-time feedback
    VISUAL_CHANGE: Enhanced filename display with metadata and copy functionality
    """
    return render_template('interface_excellence_dashboard.html')

@app.route('/dashboard')
def dashboard():
    """Dashboard principal del sistema OCR empresarial con Interface Excellence"""
    return render_template('interface_excellence_dashboard.html')

@app.route('/dashboard_old')
def dashboard_old():
    """Dashboard anterior para referencia"""
    return render_template('dashboard.html')

@app.route('/api/ocr/processed_files')
def api_get_processed_files():
    """
    FIX: Detecci√≥n corregida de archivos JSON procesados
    REASON: Usuario reporta que los archivos JSON no aparecen en la interfaz
    IMPACT: Listado correcto de todos los archivos procesados con sus JSON
    """
    try:
        from config import get_async_directories
        directories = get_async_directories()
        results_dir = directories['results']
        
        if not os.path.exists(results_dir):
            return jsonify({
                'status': 'exitoso',
                'estado': 'exitoso',
                'files': [],
                'total_files': 0,
                'message': 'No hay archivos procesados a√∫n'
            })
        
        archivos_json = []
        
        # Buscar TODOS los archivos .json en el directorio de resultados
        for archivo in os.listdir(results_dir):
            if archivo.endswith('.json'):
                archivo_path = os.path.join(results_dir, archivo)
                
                # Obtener informaci√≥n b√°sica del archivo (fuera del try para scope)
                try:
                    stat_info = os.stat(archivo_path)
                    tama√±o = stat_info.st_size
                    fecha_mod = datetime.fromtimestamp(stat_info.st_mtime)
                except OSError:
                    tama√±o = 0
                    fecha_mod = datetime.now()
                
                try:
                    # Leer contenido JSON para verificar estructura
                    with open(archivo_path, 'r', encoding='utf-8') as f:
                        contenido_json = json.load(f)
                    
                    # FIX: Detecci√≥n corregida de contenido OCR y coordenadas en estructura JSON real
                    # REASON: La funci√≥n anterior buscaba campos incorrectos causando has_ocr_data false
                    # IMPACT: Correcci√≥n de detecci√≥n de texto extra√≠do y coordenadas existentes
                    # TEST: Verificaci√≥n con estructura real de archivos JSON procesados
                    # MONITOR: Logging de campos detectados para debugging
                    # INTERFACE: Archivos procesados ahora muestran datos OCR correctamente
                    # VISUAL_CHANGE: Lista de archivos procesados muestra contenido real extra√≠do
                    # REFERENCE_INTEGRITY: Campos verificados contra estructura JSON real generada
                    
                    # Extraer datos OCR desde estructura real
                    datos_extraidos = contenido_json.get('datos_extraidos', {})
                    texto_completo = (
                        datos_extraidos.get('texto_completo', '') or
                        contenido_json.get('texto_extraido', '') or
                        contenido_json.get('full_raw_ocr_text', '')
                    )
                    
                    palabras_detectadas = (
                        datos_extraidos.get('palabras_detectadas', []) or
                        contenido_json.get('word_data', [])
                    )
                    
                    # Calcular confianza promedio desde palabras detectadas
                    confianza_promedio = 0
                    if palabras_detectadas:
                        confidencias = [p.get('confianza', 0) for p in palabras_detectadas if isinstance(p, dict)]
                        if confidencias:
                            confianza_promedio = sum(confidencias) / len(confidencias)
                    
                    # Informaci√≥n b√°sica del archivo
                    info_archivo = {
                        'filename': archivo,
                        'filepath': archivo_path,
                        'size_bytes': tama√±o,
                        'size_readable': f"{tama√±o / 1024:.1f} KB" if tama√±o > 1024 else f"{tama√±o} bytes",
                        'modified_date': fecha_mod.isoformat(),
                        'modified_readable': fecha_mod.strftime('%d/%m/%Y %H:%M:%S'),
                        'has_ocr_data': bool(texto_completo and len(texto_completo.strip()) > 0),
                        'has_coordinates': bool(palabras_detectadas and len(palabras_detectadas) > 0),
                        'word_count': len(palabras_detectadas),
                        'confidence': confianza_promedio,
                        'processing_time': contenido_json.get('tiempo_procesamiento', contenido_json.get('processing_time_ms', 0)),
                        'texto_preview': texto_completo[:100] + '...' if len(texto_completo) > 100 else texto_completo
                    }
                    
                    logger.debug(f"Archivo {archivo}: OCR={info_archivo['has_ocr_data']}, Coords={info_archivo['has_coordinates']}, Words={info_archivo['word_count']}, Texto={len(texto_completo)} chars")
                    
                    archivos_json.append(info_archivo)
                    
                except (json.JSONDecodeError, PermissionError) as e:
                    logger.warning(f"Error leyendo archivo JSON {archivo}: {e}")
                    # Incluir archivo con informaci√≥n limitada
                    archivos_json.append({
                        'filename': archivo,
                        'filepath': archivo_path,
                        'size_bytes': tama√±o,
                        'size_readable': f"{tama√±o / 1024:.1f} KB" if tama√±o > 1024 else f"{tama√±o} bytes",
                        'modified_date': fecha_mod.isoformat(),
                        'modified_readable': fecha_mod.strftime('%d/%m/%Y %H:%M:%S'),
                        'has_ocr_data': False,
                        'has_coordinates': False,
                        'error': 'Error al leer archivo'
                    })
        
        # Ordenar por fecha de modificaci√≥n (m√°s recientes primero)
        archivos_json.sort(key=lambda x: x['modified_date'], reverse=True)
        
        return jsonify({
            'status': 'exitoso',
            'estado': 'exitoso',
            'files': archivos_json,
            'total_files': len(archivos_json),
            'message': f'Se encontraron {len(archivos_json)} archivos procesados',
            'last_update': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error obteniendo archivos procesados: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'message': f'Error al obtener archivos: {str(e)}',
            'files': [],
            'total_files': 0
        }), 500

@app.route('/api/ocr/process_image', methods=['POST'])
def api_process_image():
    """
    FIX: Endpoint corregido para subida m√∫ltiple de archivos con metadatos WhatsApp
    REASON: Usuario reporta error en subida y necesita formato WhatsApp correcto
    IMPACT: Sistema funcional con metadata de WhatsApp visible en interfaz
    INTERFACE: Manejo correcto de archivos m√∫ltiples con validaci√≥n robusta
    VISUAL_CHANGE: Metadatos WhatsApp visibles en interfaz con formato correcto
    """
    try:
        # Manejo de archivos m√∫ltiples o individuales
        files_list = []
        
        if 'files' in request.files:
            files_list = request.files.getlist('files')
        elif 'images' in request.files:
            files_list = request.files.getlist('images')
        elif 'image' in request.files:
            files_list = [request.files['image']]
        else:
            return jsonify({
                'status': 'error',
                'mensaje': 'No se proporcionaron im√°genes',
                'message': 'No images provided'
            }), 400
        
        # Filtrar archivos v√°lidos
        valid_files = [f for f in files_list if f.filename != '']
        if not valid_files:
            return jsonify({
                'status': 'error',
                'mensaje': 'No hay archivos v√°lidos',
                'message': 'No valid files'
            }), 400
        
        # Obtener directorios
        from config import get_async_directories
        directories = get_async_directories()
        
        # Procesar m√∫ltiples archivos
        uploaded_files = []
        upload_results = []
        current_time = datetime.now()
        
        for file in valid_files:
            try:
                # Generar timestamp √∫nico por archivo
                timestamp_id = current_time.strftime('%Y%m%d_%H%M%S_%f')[:-3]
                
                # FIX: Manejo seguro de file.filename None/empty y validaci√≥n robusta
                # REASON: file.filename puede ser None causando NameError en operaciones de string
                # IMPACT: Previene crashes en upload cuando filename es None o inv√°lido
                # TEST: Maneja correctamente archivos sin nombre o con nombres inv√°lidos
                # MONITOR: Logging de nombres de archivos problem√°ticos
                # INTERFACE: Upload robusto que no falla con archivos problem√°ticos
                # VISUAL_CHANGE: Upload funciona consistentemente sin errores inesperados
                # REFERENCE_INTEGRITY: Validaci√≥n de None antes de operaciones de string
                
                # Validar filename seguro
                original_name = file.filename or f"upload_{timestamp_id}"
                if not isinstance(original_name, str) or len(original_name.strip()) == 0:
                    original_name = f"upload_{timestamp_id}"
                
                # Determinar extensi√≥n de archivo
                if '.' in original_name:
                    file_ext = original_name.rsplit('.', 1)[1].lower()
                    if file_ext not in ['png', 'jpg', 'jpeg']:
                        continue  # Saltar archivos no v√°lidos
                else:
                    file_ext = 'png'
                
                # Intentar detectar si es formato WhatsApp en el nombre original
                if ('--' in original_name and '@' in original_name and '_' in original_name):
                    # Es formato WhatsApp, usar nombre original con timestamp para evitar duplicados
                    base_name = original_name.rsplit('.', 1)[0]
                    final_filename = f"{base_name}_{timestamp_id}.{file_ext}"
                else:
                    # No es formato WhatsApp, generar nombre gen√©rico
                    clean_base = original_name.rsplit('.', 1)[0] if '.' in original_name else original_name
                    clean_name = secure_filename(clean_base) or f"file_{timestamp_id}"
                    final_filename = f"{clean_name}_{timestamp_id}.{file_ext}"
                
                # Guardar archivo
                file_path = os.path.join(directories['inbox'], final_filename)
                file.save(file_path)
                
                # Extraer metadatos de WhatsApp del nombre de archivo
                from app import extract_metadata_from_filename
                whatsapp_metadata = extract_metadata_from_filename(final_filename)
                
                # FIX: Priorizar metadatos del formulario sobre los extra√≠dos del nombre
                # REASON: Usuario requiere control manual de metadatos WhatsApp
                # IMPACT: Sistema utiliza datos editados por usuario en lugar de autom√°ticos
                # INTERFACE: Validaci√≥n de datos del formulario con fallback a autom√°ticos
                form_numerosorteo = request.form.get('numerosorteo', '').strip()
                form_fechasorteo = request.form.get('fechasorteo', '').strip()
                form_idWhatsapp = request.form.get('idWhatsapp', '').strip()
                form_nombre = request.form.get('nombre', '').strip()
                form_horamin = request.form.get('horamin', '').strip()
                form_caption = request.form.get('caption', '').strip()
                
                # FIX: Validaci√≥n robusta de metadatos antes del procesamiento
                # REASON: Implementar filosof√≠a Zero-Fault Detection para metadatos
                # IMPACT: Garantiza integridad de datos y previene errores en nombres
                if any([form_numerosorteo, form_fechasorteo, form_idWhatsapp, form_nombre, form_horamin]):
                    metadata_validation = validate_whatsapp_metadata({
                        'numerosorteo': form_numerosorteo,
                        'fechasorteo': form_fechasorteo,
                        'idWhatsapp': form_idWhatsapp,
                        'nombre': form_nombre,
                        'horamin': form_horamin
                    })
                    
                    if not metadata_validation['valid']:
                        logger.warning(f"Errores de validaci√≥n en metadatos: {metadata_validation['errors']}")
                        # Continuar pero registrar errores para logging
                        upload_results.append({
                            'status': 'warning',
                            'filename': file.filename,
                            'validation_errors': metadata_validation['errors'],
                            'validation_warnings': metadata_validation['warnings']
                        })
                
                # Usar datos del formulario si existen, sino usar autom√°ticos
                final_numerosorteo = form_numerosorteo or whatsapp_metadata.get('numerosorteo', 'A')
                final_fechasorteo = form_fechasorteo or current_time.strftime('%Y%m%d')
                final_idWhatsapp = form_idWhatsapp or whatsapp_metadata.get('idWhatsapp', 'unknown@lid')
                final_nombre = form_nombre or whatsapp_metadata.get('nombre', 'Unknown')
                final_horamin = form_horamin or whatsapp_metadata.get('horamin', '00-00')
                final_caption = form_caption or whatsapp_metadata.get('texto_mensaje_whatsapp', '')
                
                # Si hay metadatos del formulario, regenerar nombre de archivo
                if any([form_numerosorteo, form_fechasorteo, form_idWhatsapp, form_nombre, form_horamin]):
                    # Regenerar nombre con metadatos del formulario
                    custom_filename = f"{final_fechasorteo}-{final_numerosorteo}--{final_idWhatsapp}_{final_nombre}_{final_horamin}_{timestamp_id}.{file_ext}"
                    custom_file_path = os.path.join(directories['inbox'], custom_filename)
                    
                    # Mover archivo al nombre personalizado
                    if os.path.exists(file_path) and file_path != custom_file_path:
                        shutil.move(file_path, custom_file_path)
                        file_path = custom_file_path
                        final_filename = custom_filename
                
                # Metadatos completos con datos de WhatsApp empresariales
                metadata = {
                    'filename_original': file.filename,
                    'filename_final': final_filename,
                    'request_id': final_filename.replace(f'_{timestamp_id}', ''),
                    'upload_timestamp': current_time.isoformat(),
                    'file_size': os.path.getsize(file_path),
                    # Campos cr√≠ticos de WhatsApp con prioridad del formulario
                    'numerosorteo': final_numerosorteo,
                    'idWhatsapp': final_idWhatsapp,
                    'nombre': final_nombre,
                    'horamin': final_horamin,
                    'fechasorteo': final_fechasorteo,
                    'texto_mensaje_whatsapp': final_caption,
                    'caption': final_caption,
                    'otro_valor': request.form.get('otro_valor', 'subir_con_metadata'),
                    'form_data_used': bool(any([form_numerosorteo, form_fechasorteo, form_idWhatsapp, form_nombre, form_horamin])),
                    # Metadatos completos de WhatsApp
                    'whatsapp_metadata': whatsapp_metadata
                }
                
                # Guardar metadatos
                metadata_path = file_path.replace(f'.{file_ext}', '.metadata.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                uploaded_files.append(final_filename)
                upload_results.append({
                    'filename_original': file.filename,
                    'filename_final': final_filename,
                    'metadata': metadata,
                    'status': 'uploaded'
                })
                
                logger.info(f"‚úÖ Archivo subido: {final_filename} con metadata WhatsApp: {whatsapp_metadata}")
                
            except Exception as e:
                logger.error(f"Error subiendo archivo {file.filename}: {e}")
                upload_results.append({
                    'filename_original': file.filename,
                    'status': 'error',
                    'error': str(e)
                })
        
        if not uploaded_files:
            return jsonify({
                'status': 'error',
                'mensaje': 'No se pudieron subir archivos',
                'message': 'No files could be uploaded',
                'results': upload_results
            }), 400
        
        logger.info(f"‚úÖ {len(uploaded_files)} archivos subidos exitosamente")
        
        return jsonify({
            'status': 'success',
            'estado': 'exitoso',
            'message': f'{len(uploaded_files)} files uploaded successfully',
            'mensaje': f'{len(uploaded_files)} archivos subidos exitosamente',
            'uploaded_count': len(uploaded_files),
            'total_files': len(valid_files),
            'uploaded_files': uploaded_files,
            'results': upload_results,
            'next_steps': {
                'queue_check': '/api/ocr/queue/status',
                'process_batch': '/api/ocr/process_batch',
                'view_files': '/#queue-panel'
            }
        }), 200
        
    except Exception as e:
        logger.error(f"Error en process_image: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'mensaje': f'Error interno: {str(e)}',
            'message': f'Internal error: {str(e)}'
        }), 500

@app.route('/api/ocr/process_batch', methods=['POST'])
def api_process_batch():
    """
    FIX: Endpoint process_batch con generaci√≥n de request_id √∫nico y tracking de procesos
    REASON: JavaScript requiere request_id v√°lido para monitoreo de progreso de lote
    IMPACT: Restaura funcionalidad de tracking empresarial y monitoreo en tiempo real
    TEST: Verifica generaci√≥n UUID √∫nico y respuesta JSON v√°lida con request_id
    MONITOR: Logging detallado de inicio y progreso de procesamiento por lotes
    INTERFACE: JavaScript recibe request_id v√°lido para mostrar progreso en tiempo real
    VISUAL_CHANGE: Progreso de procesamiento visible con request_id v√°lido
    REFERENCE_INTEGRITY: request_id generado es √∫nico y persistente durante procesamiento
    """
    try:
        import uuid
        from datetime import datetime
        
        # Generar request_id √∫nico para el lote
        batch_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_uuid = str(uuid.uuid4())[:8]
        request_id = f"BATCH_{batch_timestamp}_{batch_uuid}"
        
        # FIX: Manejo robusto de datos JSON y form-data
        # REASON: Error 400 puede ser causado por datos malformados o contenido mixto
        # IMPACT: Procesamiento estable independiente del tipo de contenido
        # TEST: Maneja tanto JSON como form-data correctamente
        # MONITOR: Logging detallado del tipo de contenido recibido
        # INTERFACE: Compatible con diferentes tipos de requests desde frontend
        # VISUAL_CHANGE: Procesamiento exitoso elimina errores 400
        # REFERENCE_INTEGRITY: Validaci√≥n de tipos de datos antes de procesamiento
        
        data = {}
        content_type = request.content_type or ''
        
        if 'application/json' in content_type:
            data = request.get_json() or {}
        elif 'application/x-www-form-urlencoded' in content_type or 'multipart/form-data' in content_type:
            data = request.form.to_dict()
        else:
            # Intentar ambos m√©todos
            try:
                data = request.get_json() or {}
            except:
                data = request.form.to_dict() if request.form else {}
        
        logger.info(f"üì® Procesando lote con datos: {data}")
        logger.info(f"üìù Content-Type: {content_type}")
        
        # Obtener configuraci√≥n con valores por defecto
        profile = data.get('profile', 'ultra_rapido')
        batch_size = int(data.get('batch_size', 5))
        
        logger.info(f"‚úÖ Procesamiento de lote iniciado. Request ID: {request_id}")
        logger.info(f"‚öôÔ∏è Configuraci√≥n: profile={profile}, batch_size={batch_size}")
        
        # Inicializar orquestador
        orquestador = OrquestadorOCR()
        
        # Procesar lote con tracking del request_id
        resultado = orquestador.process_queue_batch(
            max_files=batch_size,
            profile=profile
        )
        
        # A√±adir request_id al resultado
        if isinstance(resultado, dict):
            resultado['request_id'] = request_id
            resultado['batch_id'] = request_id
            resultado['processing_status'] = 'completed'
            resultado['timestamp'] = datetime.now().isoformat()
        else:
            resultado = {
                'status': 'success',
                'request_id': request_id,
                'batch_id': request_id,
                'processing_status': 'completed',
                'timestamp': datetime.now().isoformat(),
                'resultado_original': resultado
            }
        
        processed_count = resultado.get('batch_info', {}).get('processed_count', 0)
        
        # FIX: ALMACENAR REQUEST_ID DEL √öLTIMO LOTE PROCESADO PARA FILTRADO
        # REASON: Usuario requiere JSON consolidado espec√≠fico del √∫ltimo lote √∫nicamente
        # IMPACT: Permite filtrado preciso de archivos por lote para evitar mezcla de resultados
        if processed_count > 0:
            _store_last_batch_request_id(request_id)
            logger.info(f"üíæ Almacenado request_id del √∫ltimo lote exitoso: {request_id}")
        
        logger.info(f"‚úÖ Lote procesado exitosamente: {processed_count} archivos. Request ID: {request_id}")
        
        return jsonify(resultado)
        
    except Exception as e:
        # FIX: Manejo seguro de request_id en caso de error antes de su definici√≥n
        # REASON: Variable request_id puede no estar definida si error ocurre antes
        # IMPACT: Evita NameError en manejo de errores cr√≠ticos
        # TEST: Error handling funciona correctamente en cualquier punto del flujo
        # MONITOR: Logging de errores sin fallos adicionales
        # INTERFACE: Respuestas de error consistentes y robustas
        # VISUAL_CHANGE: Errores manejados sin crashes adicionales
        # REFERENCE_INTEGRITY: Variable request_id manejada de forma segura
        safe_request_id = locals().get('request_id', f"ERROR_{int(time.time())}")
        logger.error(f"Error cr√≠tico procesando lote: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'mensaje': f'Error al procesar el lote: {str(e)}',
            'message': f'Batch processing error: {str(e)}',
            'request_id': safe_request_id,
            'error_code': 'BATCH_PROCESSING_ERROR'
        }), 500

@app.route('/api/ocr/result/<request_id>')
def api_get_result(request_id):
    """Obtener resultado individual"""
    try:
        from config import get_async_directories
        directories = get_async_directories()
        
        # Buscar archivo de resultado
        result_file = os.path.join(directories['results'], f"{request_id}.json")
        
        if not os.path.exists(result_file):
            return jsonify({
                'status': 'not_found',
                'estado': 'no_encontrado',
                'mensaje': 'Resultado no encontrado',
                'message': 'Result not found'
            }), 404
        
        # Leer resultado
        with open(result_file, 'r', encoding='utf-8') as f:
            resultado = json.load(f)
        
        return jsonify(resultado)
        
    except Exception as e:
        logger.error(f"Error obteniendo resultado: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'mensaje': f'Error al obtener resultado: {str(e)}',
            'message': f'Error getting result: {str(e)}'
        }), 500

@app.route('/api/ocr/queue/status')
def api_queue_status():
    """
    FIX: Estado de la cola de procesamiento con verificaci√≥n de directorios
    REASON: Evitar errores cuando directorios no existen durante inicializaci√≥n
    IMPACT: Sistema robusto que funciona correctamente desde el primer arranque
    """
    try:
        from config import get_async_directories
        directories = get_async_directories()
        
        # FIX: Crear directorios si no existen
        for dir_path in directories.values():
            os.makedirs(dir_path, exist_ok=True)
        
        # Contar archivos en cada directorio con manejo de errores
        inbox_count = 0
        processing_count = 0
        processed_count = 0
        results_count = 0
        errors_count = 0
        
        try:
            inbox_count = len([f for f in os.listdir(directories['inbox']) if f.endswith(('.png', '.jpg', '.jpeg'))])
        except:
            pass
            
        try:
            processing_count = len([f for f in os.listdir(directories['processing']) if f.endswith(('.png', '.jpg', '.jpeg'))])
        except:
            pass
            
        try:
            processed_count = len([f for f in os.listdir(directories['processed']) if f.endswith(('.png', '.jpg', '.jpeg'))])
        except:
            pass
            
        try:
            results_count = len([f for f in os.listdir(directories['results']) if f.endswith('.json')])
        except:
            pass
            
        try:
            errors_count = len([f for f in os.listdir(directories.get('errors', 'data/errors')) if f.endswith(('.png', '.jpg', '.jpeg'))])
        except:
            pass
        
        # FIX: Agregar lista de archivos en cola con metadatos WhatsApp
        # REASON: Usuario reporta que no se ven archivos en cola
        # IMPACT: Interfaz muestra archivos reales con metadatos completos
        # VISUAL_CHANGE: Archivos visibles en interfaz con nombres completos de WhatsApp
        
        inbox_files = []
        try:
            for filename in os.listdir(directories['inbox']):
                if filename.endswith(('.png', '.jpg', '.jpeg')):
                    file_path = os.path.join(directories['inbox'], filename)
                    metadata_path = file_path.replace(f'.{filename.split(".")[-1]}', '.metadata.json')
                    
                    file_info = {
                        'filename': filename,
                        'size': os.path.getsize(file_path),
                        'upload_time': os.path.getmtime(file_path)
                    }
                    
                    # Cargar metadatos si existen
                    if os.path.exists(metadata_path):
                        try:
                            with open(metadata_path, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                                file_info.update({
                                    'numerosorteo': metadata.get('numerosorteo', 'A'),
                                    'idWhatsapp': metadata.get('idWhatsapp', 'unknown@lid'),
                                    'nombre': metadata.get('nombre', 'Unknown'),
                                    'horamin': metadata.get('horamin', '00-00'),
                                    'caption': metadata.get('caption', ''),
                                    'whatsapp_metadata': metadata.get('whatsapp_metadata', {})
                                })
                        except Exception as e:
                            logger.warning(f"Error leyendo metadata para {filename}: {e}")
                    
                    inbox_files.append(file_info)
        except Exception as e:
            logger.warning(f"Error obteniendo archivos en cola: {e}")
        
        return jsonify({
            'status': 'ok',
            'estado': 'exitoso',
            'inbox_count': inbox_count,
            'processing_count': processing_count,
            'processed_count': processed_count,
            'error_count': errors_count,
            'inbox_files': inbox_files,
            'queue_status': {
                'inbox': inbox_count,
                'pending': inbox_count,
                'processing': processing_count,
                'completed': processed_count,
                'processed': processed_count,
                'results_available': results_count,
                'errors': errors_count
            },
            'system_status': {
                'ocr_loaded': getattr(app_module, '_ocr_components_loaded', False),
                'worker_running': getattr(app_module, '_worker_running', False)
            },
            'total_active': inbox_count + processing_count,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error obteniendo estado de cola: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'mensaje': f'Error al obtener estado: {str(e)}',
            'message': f'Error getting queue status: {str(e)}'
        }), 500



@app.route('/api/ocr/processed_files')
def api_processed_files():
    """
    FIX: Endpoint para obtener lista de archivos procesados con sus resultados
    REASON: Usuario necesita ver lista de archivos procesados con selectores y visor
    IMPACT: Interface mejorada para gesti√≥n de resultados OCR
    """
    try:
        from config import get_async_directories
        directories = get_async_directories()
        
        processed_files = []
        results_dir = Path(directories['results'])
        processed_dir = Path(directories['processed'])
        
        # Crear directorios si no existen
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(processed_dir, exist_ok=True)
        
        # Buscar archivos JSON de resultados con mapeo inteligente
        for json_file in results_dir.glob('*.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # FIX: Algoritmo inteligente de mapeo archivo-resultado
                # REASON: Sistema actual no puede mapear BATCH_ prefijos con archivos originales
                # IMPACT: Correlaci√≥n correcta entre archivos procesados y resultados JSON
                image_name, image_path = _find_corresponding_image(json_file, result_data, processed_dir)
                
                if not image_path:
                    # Busqueda fallback usando nombre del JSON
                    fallback_name = json_file.stem
                    for ext in ['.png', '.jpg', '.jpeg']:
                        potential_path = processed_dir / f"{fallback_name}{ext}"
                        if potential_path.exists():
                            image_path = str(potential_path.relative_to(Path.cwd()))
                            image_name = fallback_name
                            break
                
                # FIX: Estructura de datos completa para prevenir valores "undefined"
                # REASON: Frontend esperaba campos espec√≠ficos que no exist√≠an
                # IMPACT: Datos consistentes y completos en todas las respuestas
                # TEST: Validaci√≥n de todos los campos requeridos por frontend
                # MONITOR: Logging de campos faltantes para debugging
                # INTERFACE: Estructura de datos compatible con dashboard
                # VISUAL_CHANGE: Informaci√≥n correcta visible en lugar de "undefined"
                # REFERENCE_INTEGRITY: Todos los campos validados y con valores por defecto
                
                # Extraer estad√≠sticas de procesamiento
                calidad_extraccion = result_data.get('calidad_extraccion', {})
                estadisticas = result_data.get('estadisticas', {})
                datos_extraidos = result_data.get('datos_extraidos', {})
                
                # Calcular tiempo de procesamiento
                tiempo_procesamiento = result_data.get('tiempo_procesamiento', 0)
                tiempo_procesamiento_formatted = f"{tiempo_procesamiento:.2f}s" if tiempo_procesamiento > 0 else "N/A"
                
                # Extraer palabras detectadas
                palabras_detectadas = estadisticas.get('total_palabras', 0)
                if palabras_detectadas == 0:
                    # Intentar extraer de otras fuentes
                    palabras_detectadas = len(datos_extraidos.get('palabras_individuales', []))
                
                # Extraer confianza promedio
                confianza_promedio = calidad_extraccion.get('confianza_promedio', 0)
                if confianza_promedio == 0:
                    confianza_promedio = estadisticas.get('confianza_promedio', 0)
                
                # Formatear confianza
                confianza_formatted = f"{confianza_promedio:.1f}%" if confianza_promedio > 0 else "N/A"
                
                # Extraer texto completo
                texto_completo = datos_extraidos.get('texto_completo', '')
                if not texto_completo:
                    texto_completo = result_data.get('texto_extraido', '')
                
                processed_files.append({
                    'filename': json_file.name,
                    'image_name': image_name or 'Sin_nombre',
                    'image_path': image_path or '',
                    'json_path': str(json_file.relative_to(Path.cwd())),
                    'result_data': result_data,
                    'processed_date': datetime.fromtimestamp(json_file.stat().st_mtime).isoformat(),
                    'file_size': json_file.stat().st_size,
                    'has_coordinates': 'coordenadas' in datos_extraidos,
                    'extraction_quality': calidad_extraccion.get('categoria', 'N/A'),
                    
                    # FIX: Campos espec√≠ficos para frontend
                    'tiempo_procesamiento': tiempo_procesamiento_formatted,
                    'palabras_detectadas': palabras_detectadas,
                    'confianza_promedio': confianza_formatted,
                    'texto_completo': texto_completo[:200] + '...' if len(texto_completo) > 200 else texto_completo,
                    
                    # FIX: Campos de estado para interface
                    'estado': 'completado',
                    'disponible': True,
                    'error': None,
                    
                    # FIX: Metadata para visualizaci√≥n
                    'metadata': {
                        'original_filename': result_data.get('archivo_info', {}).get('original_filename', image_name or 'Sin_nombre'),
                        'formato': result_data.get('archivo_info', {}).get('formato', 'N/A'),
                        'tama√±o_imagen': result_data.get('archivo_info', {}).get('tama√±o', 'N/A'),
                        'fecha_creacion': result_data.get('archivo_info', {}).get('fecha_creacion', 'N/A')
                    }
                })
                
            except Exception as e:
                logger.warning(f"Error procesando archivo {json_file}: {e}")
                continue
        
        # Ordenar por fecha de procesamiento
        processed_files.sort(key=lambda x: x['processed_date'], reverse=True)
        
        return jsonify({
            'status': 'exitoso',
            'estado': 'exitoso',
            'processed_files': processed_files,
            'total_files': len(processed_files),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error obteniendo archivos procesados: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'mensaje': f'Error al obtener archivos procesados: {str(e)}',
            'message': f'Error getting processed files: {str(e)}'
        }), 500

@app.route('/api/ocr/result_data/<filename>')
def get_result_data(filename):
    """
    FIX: Endpoint espec√≠fico para datos de resultados del visualizador
    REASON: Frontend necesita datos estructurados para el visualizador de resultados
    IMPACT: Elimina valores "undefined" en el visualizador y proporciona datos completos
    TEST: Verifica estructura de datos espec√≠fica para visualizador
    MONITOR: Logging de requests de visualizador para debugging
    INTERFACE: Datos estructurados espec√≠ficos para componentes de visualizaci√≥n
    VISUAL_CHANGE: Visualizador muestra datos reales en lugar de "undefined"
    REFERENCE_INTEGRITY: Validaci√≥n de existencia de archivos antes de respuesta
    """
    try:
        from config import get_async_directories
        directories = get_async_directories()
        
        # FIX: B√∫squeda inteligente de archivos JSON con m√∫ltiples estrategias
        # REASON: Los nombres de archivos pueden tener diferentes formatos (.png.json vs .json)
        # IMPACT: Encuentra archivos JSON correctamente independiente del formato de nombre
        # TEST: Busca con m√∫ltiples patrones de nombres posibles
        # MONITOR: Logging de estrategias de b√∫squeda para debugging
        # INTERFACE: Compatibilidad con diferentes formatos de nombres desde frontend
        # VISUAL_CHANGE: Visualizador encuentra datos correctamente
        # REFERENCE_INTEGRITY: Validaci√≥n exhaustiva de existencia de archivos
        
        results_dir = Path(directories['results'])
        json_path = None
        searched_files = []
        
        # Estrategia 1: Nombre directo si ya termina en .json
        if filename.endswith('.json'):
            direct_path = results_dir / filename
            searched_files.append(filename)
            if direct_path.exists():
                json_path = direct_path
        
        # Estrategia 2: Agregar .json si no lo tiene
        if not json_path:
            json_filename = filename if filename.endswith('.json') else f"{filename}.json"
            candidate_path = results_dir / json_filename
            searched_files.append(json_filename)
            if candidate_path.exists():
                json_path = candidate_path
        
        # Estrategia 3: Reemplazar extensi√≥n con .json
        if not json_path:
            base_name = filename.rsplit('.', 1)[0] if '.' in filename else filename
            json_filename = f"{base_name}.json"
            candidate_path = results_dir / json_filename
            searched_files.append(json_filename)
            if candidate_path.exists():
                json_path = candidate_path
        
        # Estrategia 4: Buscar por patr√≥n fuzzy
        if not json_path:
            import re
            # Extraer parte base del nombre sin extensiones
            base_pattern = filename.replace('.png', '').replace('.jpg', '').replace('.jpeg', '').replace('.json', '')
            
            for json_file in results_dir.glob('*.json'):
                if base_pattern in json_file.name:
                    json_path = json_file
                    searched_files.append(json_file.name)
                    break
        
        # Si no se encuentra, buscar en directorio de procesados
        if not json_path:
            processed_dir = Path(directories['processed'])
            for strategy_name in searched_files:
                candidate_path = processed_dir / strategy_name
                if candidate_path.exists():
                    json_path = candidate_path
                    break
        
        if not json_path:
            return jsonify({
                'status': 'error',
                'message': 'Archivo de resultados no encontrado',
                'filename': filename,
                'searched_files': searched_files,
                'searched_directories': [str(results_dir), str(Path(directories['processed']))]
            }), 404
        
        # Leer datos del archivo JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            result_data = json.load(f)
        
        # FIX: Estructura espec√≠fica para visualizador
        # REASON: Visualizador requiere campos espec√≠ficos para mostrar datos
        # IMPACT: Datos consistentes y completos en visualizador
        # TEST: Todos los campos del visualizador tienen valores v√°lidos
        # MONITOR: Logging de datos enviados al visualizador
        # INTERFACE: Estructura optimizada para componentes de visualizaci√≥n
        # VISUAL_CHANGE: Visualizador muestra informaci√≥n real y detallada
        # REFERENCE_INTEGRITY: Validaci√≥n de estructura de datos antes de env√≠o
        
        calidad_extraccion = result_data.get('calidad_extraccion', {})
        estadisticas = result_data.get('estadisticas', {})
        datos_extraidos = result_data.get('datos_extraidos', {})
        archivo_info = result_data.get('archivo_info', {})
        
        # FIX: Extracci√≥n robusta de texto desde m√∫ltiples ubicaciones posibles
        # REASON: Archivos de cach√© tienen estructura diferente que no est√° siendo detectada
        # IMPACT: Visualizador muestra texto real extra√≠do por OCR en lugar de estar vac√≠o
        # TEST: Busca texto en todas las ubicaciones posibles del JSON
        # MONITOR: Logging de ubicaci√≥n encontrada para debugging
        # INTERFACE: Visualizador muestra texto completo extra√≠do
        # VISUAL_CHANGE: Texto OCR ahora visible en lugar de campo vac√≠o
        # REFERENCE_INTEGRITY: Validaci√≥n de datos extra√≠dos antes de env√≠o
        
        # Buscar texto en m√∫ltiples ubicaciones
        texto_completo = ''
        texto_sources = [
            datos_extraidos.get('texto_completo', ''),
            result_data.get('texto_extraido', ''),
            result_data.get('ocr_data', {}).get('texto_completo', ''),
            result_data.get('ocr_data', {}).get('raw_text', ''),
            result_data.get('raw_text', '')
        ]
        
        for source in texto_sources:
            if source and isinstance(source, str) and len(source.strip()) > 0:
                texto_completo = source.strip()
                logger.debug(f"Texto encontrado en fuente: {len(texto_completo)} caracteres")
                break
        
        # Extraer coordenadas desde m√∫ltiples ubicaciones
        coordenadas = []
        coordenadas_sources = [
            datos_extraidos.get('coordenadas', []),
            result_data.get('ocr_data', {}).get('coordenadas', []),
            result_data.get('coordenadas', []),
            result_data.get('word_coordinates', [])
        ]
        
        for source in coordenadas_sources:
            if source and isinstance(source, list) and len(source) > 0:
                coordenadas = source
                break
        
        # Extraer palabras individuales
        palabras_individuales = []
        palabras_sources = [
            datos_extraidos.get('palabras_individuales', []),
            result_data.get('ocr_data', {}).get('palabras_individuales', []),
            result_data.get('palabras_individuales', []),
            result_data.get('word_data', [])
        ]
        
        for source in palabras_sources:
            if source and isinstance(source, list) and len(source) > 0:
                palabras_individuales = source
                break
        
        # Extraer datos financieros
        datos_financieros = datos_extraidos.get('datos_financieros', {})
        if not datos_financieros:
            datos_financieros = result_data.get('datos_financieros', {})
        
        # Preparar respuesta estructurada
        viewer_data = {
            'status': 'success',
            'filename': filename,
            'archivo_info': {
                'nombre_original': archivo_info.get('original_filename', filename),
                'formato': archivo_info.get('formato', 'N/A'),
                'tama√±o': archivo_info.get('tama√±o', 'N/A'),
                'fecha_procesamiento': result_data.get('fecha_procesamiento', 'N/A')
            },
            'estadisticas': {
                'tiempo_procesamiento': f"{result_data.get('tiempo_procesamiento', 0):.2f}s",
                'total_palabras': estadisticas.get('total_palabras', len(palabras_individuales)),
                'confianza_promedio': f"{calidad_extraccion.get('confianza_promedio', 0):.1f}%",
                'calidad_categoria': calidad_extraccion.get('categoria', 'N/A'),
                'palabras_alta_confianza': estadisticas.get('palabras_alta_confianza', 0),
                'palabras_baja_confianza': estadisticas.get('palabras_baja_confianza', 0)
            },
            'texto_extraido': {
                'texto_completo': texto_completo,
                'longitud_texto': len(texto_completo),
                'lineas_texto': len(texto_completo.split('\n')) if texto_completo else 0
            },
            'coordenadas': {
                'disponibles': len(coordenadas) > 0,
                'total': len(coordenadas),
                'elementos': coordenadas[:50]  # Limitar para performance
            },
            'palabras_individuales': {
                'total': len(palabras_individuales),
                'con_coordenadas': len([p for p in palabras_individuales if p.get('coordenadas')]),
                'elementos': palabras_individuales[:50]  # Limitar para performance
            },
            'datos_financieros': {
                'disponibles': len(datos_financieros) > 0,
                'campos_detectados': list(datos_financieros.keys()) if datos_financieros else [],
                'datos': datos_financieros
            },
            'calidad_extraccion': {
                'score': calidad_extraccion.get('score', 0),
                'categoria': calidad_extraccion.get('categoria', 'N/A'),
                'confianza_promedio': calidad_extraccion.get('confianza_promedio', 0),
                'distribucion_confianza': calidad_extraccion.get('distribucion_confianza', {}),
                'recomendaciones': calidad_extraccion.get('recomendaciones', [])
            }
        }
        
        return jsonify(viewer_data)
        
    except Exception as e:
        logger.error(f"Error obteniendo datos para visualizador {filename}: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error obteniendo datos del visualizador: {str(e)}',
            'filename': filename
        }), 500

@app.route('/api/ocr/download_json/<filename>')
def download_json(filename):
    """
    FIX: Endpoint para descargar archivos JSON de resultados OCR
    REASON: Usuario necesita poder descargar resultados JSON individualmente
    IMPACT: Permite acceso directo a resultados de OCR procesados
    """
    try:
        from config import get_async_directories
        directories = get_async_directories()
        
        # Buscar archivo JSON en directorio de resultados
        results_dir = Path(directories['results'])
        json_filename = filename.replace('.png', '.json')
        json_path = results_dir / json_filename
        
        if not json_path.exists():
            # Buscar tambi√©n en directorio de procesados
            processed_dir = Path(directories['processed'])
            json_path = processed_dir / json_filename
            
        if not json_path.exists():
            return jsonify({
                'status': 'error',
                'message': 'Archivo JSON no encontrado'
            }), 404
        
        # Leer y devolver el archivo JSON
        from flask import send_file
        return send_file(
            json_path,
            as_attachment=True,
            download_name=json_filename,
            mimetype='application/json'
        )
        
    except Exception as e:
        logger.error(f"Error descargando JSON {filename}: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error descargando archivo: {str(e)}'
        }), 500

# Sistema de gesti√≥n de API Keys
API_KEYS_FILE = 'api_keys.json'

def load_api_keys():
    """Cargar API keys desde archivo"""
    if os.path.exists(API_KEYS_FILE):
        try:
            with open(API_KEYS_FILE, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_api_keys(keys_data):
    """Guardar API keys en archivo"""
    with open(API_KEYS_FILE, 'w') as f:
        json.dump(keys_data, f, indent=2)

def generate_api_key():
    """Generar una nueva API key √∫nica"""
    import secrets
    return f"ocr_{''.join(secrets.choice('abcdefghijklmnopqrstuvwxyz0123456789') for _ in range(32))}"

@app.route('/api/generate_key', methods=['POST'])
def api_generate_key():
    """
    FIX: Endpoint para generar nuevas API keys con permisos configurables
    REASON: Usuario requiere sistema de generaci√≥n de API keys para uso externo
    IMPACT: Permite autenticaci√≥n segura para integraciones externas del sistema OCR
    """
    try:
        data = request.get_json()
        name = data.get('name', '').strip()
        permissions = data.get('permissions', {})
        
        if not name:
            return jsonify({
                'status': 'error',
                'message': 'Nombre requerido para la API key'
            }), 400
            
        # Generar nueva API key
        api_key = generate_api_key()
        key_id = str(uuid.uuid4())
        
        # Cargar keys existentes
        keys_data = load_api_keys()
        
        # A√±adir nueva key
        keys_data[key_id] = {
            'id': key_id,
            'name': name,
            'key': api_key,
            'permissions': permissions,
            'created_at': datetime.now().isoformat(),
            'last_used': None,
            'usage_count': 0
        }
        
        # Guardar
        save_api_keys(keys_data)
        
        logger.info(f"Nueva API key generada: {name} ({key_id})")
        
        return jsonify({
            'status': 'success',
            'api_key': api_key,
            'key_id': key_id,
            'message': 'API key generada exitosamente'
        })
        
    except Exception as e:
        logger.error(f"Error generando API key: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error generando API key: {str(e)}'
        }), 500

@app.route('/api/list_keys')
def api_list_keys():
    """
    FIX: Endpoint para listar API keys existentes (sin mostrar la key)
    REASON: Usuario necesita ver y gestionar API keys creadas
    IMPACT: Interface de gesti√≥n completa para API keys del sistema
    """
    try:
        keys_data = load_api_keys()
        
        # Preparar respuesta sin exponer las keys
        keys_list = []
        for key_info in keys_data.values():
            keys_list.append({
                'id': key_info['id'],
                'name': key_info['name'],
                'created_at': key_info['created_at'],
                'last_used': key_info.get('last_used'),
                'usage_count': key_info.get('usage_count', 0),
                'permissions': key_info.get('permissions', {})
            })
        
        return jsonify({
            'status': 'success',
            'keys': keys_list
        })
        
    except Exception as e:
        logger.error(f"Error listando API keys: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error listando API keys: {str(e)}'
        }), 500

@app.route('/api/revoke_key/<key_id>', methods=['DELETE'])
def api_revoke_key(key_id):
    """
    FIX: Endpoint para revocar API keys espec√≠ficas
    REASON: Usuario necesita poder eliminar API keys no utilizadas o comprometidas
    IMPACT: Gesti√≥n segura completa del ciclo de vida de API keys
    """
    try:
        keys_data = load_api_keys()
        
        if key_id not in keys_data:
            return jsonify({
                'status': 'error',
                'message': 'API key no encontrada'
            }), 404
            
        # Remover la key
        removed_key = keys_data.pop(key_id)
        save_api_keys(keys_data)
        
        logger.info(f"API key revocada: {removed_key['name']} ({key_id})")
        
        return jsonify({
            'status': 'success',
            'message': 'API key revocada exitosamente'
        })
        
    except Exception as e:
        logger.error(f"Error revocando API key: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error revocando API key: {str(e)}'
        }), 500

@app.route('/api/clean_queue', methods=['POST'])
def api_clean_queue_new():
    """
    FIX: Endpoint espec√≠fico para limpiar cola de archivos no procesados
    REASON: Usuario requiere limpieza selectiva de cola sin afectar procesados
    IMPACT: Permite limpiar solo archivos pendientes manteniendo historial procesado
    TEST: Limpia solo directorio inbox sin tocar processed/results
    MONITOR: Logging espec√≠fico de limpieza de cola con contadores
    INTERFACE: Llamado por bot√≥n "Limpiar Cola" en secci√≥n de archivos en cola
    VISUAL_CHANGE: Limpia lista de cola sin afectar archivos procesados
    REFERENCE_INTEGRITY: Endpoint /api/clean_queue espec√≠fico para gesti√≥n de cola
    """
    try:
        from config import get_async_directories
        import glob
        
        directories = get_async_directories()
        
        # FIX: Limpiar solo directorio inbox (cola de archivos)
        # REASON: Usuario requiere limpieza selectiva sin afectar archivos procesados
        # IMPACT: Mantiene historial de procesados mientras limpia pendientes
        inbox_files = glob.glob(os.path.join(directories['inbox'], "*.*"))
        inbox_cleaned = 0
        for file_path in inbox_files:
            try:
                os.remove(file_path)
                inbox_cleaned += 1
            except Exception as e:
                logger.warning(f"No se pudo eliminar archivo de cola {file_path}: {e}")
        
        logger.info(f"‚úÖ Cola de archivos limpiada: {inbox_cleaned} archivos eliminados")
        
        return jsonify({
            'status': 'exitoso',
            'estado': 'exitoso',
            'message': f'Cola limpiada exitosamente: {inbox_cleaned} archivos eliminados',
            'mensaje': f'Cola limpiada exitosamente: {inbox_cleaned} archivos eliminados',
            'inbox_cleaned': inbox_cleaned,
            'timestamp': datetime.now().isoformat(),
            'success': True
        }), 200
        
    except Exception as e:
        logger.error(f"Error cr√≠tico limpiando cola: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'message': f'Error al limpiar cola: {str(e)}',
            'error_code': 'QUEUE_CLEAN_ERROR'
        }), 500

@app.route('/api/clean', methods=['POST'])
def api_clean_system():
    """
    FIX: Endpoint cr√≠tico para limpiar el sistema despu√©s de procesar lotes
    REASON: Interface llama a /api/clean que estaba faltante en routes.py principal
    IMPACT: Botones de limpieza completamente funcionales siguiendo workflow empresarial
    TEST: Limpia directorios processed, results, errors y temporales correctamente
    MONITOR: Logging detallado de archivos eliminados y contadores por directorio
    INTERFACE: Endpoint llamado por botones de limpieza en interface_excellence_dashboard.html
    VISUAL_CHANGE: Habilita funcionalidad completa de limpieza en workflow empresarial
    REFERENCE_INTEGRITY: Endpoint /api/clean ahora existe y es accesible desde frontend
    """
    try:
        from config import get_async_directories
        import glob
        import shutil
        
        directories = get_async_directories()
        cleaned_counts = {}
        
        # FIX: Mover archivos procesados a HISTORIAL en lugar de eliminarlos
        # REASON: Usuario requiere historial para preservar archivos sin interferir con nuevos lotes
        # IMPACT: Limpieza segura que preserva archivos en historial empresarial permanente
        # WORKFLOW: processed ‚Üí historial ‚Üí eliminaci√≥n tras 24h en historial solamente
        # INTEGRIDAD: Archivos procesados NO interfieren pero se preservan en historial
        processed_files = glob.glob(os.path.join(directories['processed'], "*.*"))
        processed_moved = 0
        historial_dir = 'data/historial'
        # Crear directorio historial si no existe
        os.makedirs(historial_dir, exist_ok=True)
        
        for file_path in processed_files:
            try:
                filename = os.path.basename(file_path)
                # Agregar timestamp al archivo en historial para evitar conflictos
                name, ext = os.path.splitext(filename)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                historial_filename = f"{name}_historial_{timestamp}{ext}"
                historial_path = os.path.join(historial_dir, historial_filename)
                
                # Mover archivo a historial
                shutil.move(file_path, historial_path)
                processed_moved += 1
                logger.debug(f"Archivo movido a historial: {filename} ‚Üí {historial_filename}")
            except Exception as e:
                logger.warning(f"No se pudo mover archivo procesado {file_path}: {e}")
        cleaned_counts['processed_moved_to_historial'] = processed_moved
        
        # FIX: Mover archivos results a HISTORIAL y aplicar eliminaci√≥n 24h SOLO en historial
        # REASON: Usuario requiere que archivos procesados vayan a historial, eliminaci√≥n 24h solo en historial
        # IMPACT: Results activos se mueven a historial, eliminaci√≥n temporal SOLO en historial
        # WORKFLOW: results ‚Üí historial ‚Üí eliminaci√≥n tras 24h SOLO en historial
        # INTEGRIDAD: Results no acumulan, historial maneja retenci√≥n temporal
        
        from datetime import datetime, timedelta
        
        # Mover archivos results a historial
        result_files = glob.glob(os.path.join(directories['results'], "*.json"))
        results_moved = 0
        
        for file_path in result_files:
            try:
                filename = os.path.basename(file_path)
                # Agregar timestamp para evitar conflictos en historial
                name, ext = os.path.splitext(filename)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                historial_filename = f"{name}_result_{timestamp}{ext}"
                historial_path = os.path.join(historial_dir, historial_filename)
                
                # Mover a historial
                shutil.move(file_path, historial_path)
                results_moved += 1
                logger.debug(f"Resultado movido a historial: {filename} ‚Üí {historial_filename}")
            except Exception as e:
                logger.warning(f"No se pudo mover resultado {file_path}: {e}")
        
        cleaned_counts['results_moved_to_historial'] = results_moved
        
        # FIX: Aplicar eliminaci√≥n de 24 horas SOLO en directorio historial
        # REASON: Historial mantiene archivos 24h antes de eliminaci√≥n final
        # IMPACT: Eliminaci√≥n temporal solo afecta historial, no archivos activos
        historial_files = glob.glob(os.path.join(historial_dir, "*.*"))
        historial_cleaned = 0
        historial_preserved = 0
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        for file_path in historial_files:
            try:
                file_stat = os.stat(file_path)
                file_time = datetime.fromtimestamp(file_stat.st_mtime)
                
                if file_time < cutoff_time:
                    # Archivo en historial tiene m√°s de 24 horas, eliminar definitivamente
                    os.remove(file_path)
                    historial_cleaned += 1
                    logger.debug(f"Archivo historial eliminado (>24h): {os.path.basename(file_path)}")
                else:
                    # Archivo reciente en historial, preservar
                    historial_preserved += 1
                    logger.debug(f"Archivo historial preservado (<24h): {os.path.basename(file_path)}")
                    
            except Exception as e:
                logger.warning(f"No se pudo procesar archivo historial {file_path}: {e}")
                
        cleaned_counts['historial_eliminated'] = historial_cleaned
        cleaned_counts['historial_preserved'] = historial_preserved
        
        if historial_preserved > 0:
            logger.info(f"üìÅ Historial empresarial: {historial_preserved} preservados, {historial_cleaned} eliminados (>24h)")
        
        # FIX: Limpiar directorio errors (opcional para archivos con errores)
        # REASON: Limpiar archivos que fallaron en procesamiento anterior
        # IMPACT: Sistema libre de archivos problem√°ticos acumulados
        try:
            error_files = glob.glob(os.path.join(directories.get('errors', 'data/errors'), "*.*"))
            errors_cleaned = 0
            for file_path in error_files:
                try:
                    os.remove(file_path)
                    errors_cleaned += 1
                except Exception as e:
                    logger.warning(f"No se pudo eliminar error {file_path}: {e}")
            cleaned_counts['errors'] = errors_cleaned
        except Exception as e:
            logger.warning(f"Error accediendo directorio de errores: {e}")
            cleaned_counts['errors'] = 0
        
        # FIX: Limpiar directorios temporales web
        # REASON: Eliminar directorios temporales creados durante procesamiento web
        # IMPACT: Libera espacio y evita acumulaci√≥n de archivos temporales
        temp_dirs = glob.glob(os.path.join('temp', 'web_*'))
        temp_cleaned = 0
        for temp_dir in temp_dirs:
            try:
                shutil.rmtree(temp_dir)
                temp_cleaned += 1
            except Exception as e:
                logger.warning(f"No se pudo eliminar directorio temporal {temp_dir}: {e}")
        cleaned_counts['temp_dirs'] = temp_cleaned
        
        total_cleaned = sum(cleaned_counts.values())
        logger.info(f"‚úÖ Sistema limpiado exitosamente: {cleaned_counts} - Total: {total_cleaned} elementos")
        
        return jsonify({
            'status': 'exitoso',
            'estado': 'exitoso',
            'message': 'Sistema limpiado exitosamente',
            'mensaje': 'Sistema limpiado exitosamente',
            'cleaned_counts': cleaned_counts,
            'total_cleaned': total_cleaned,
            'timestamp': datetime.now().isoformat(),
            'success': True
        }), 200
        
    except Exception as e:
        logger.error(f"Error cr√≠tico limpiando sistema: {e}")
        return jsonify({
            'status': 'error',
            'estado': 'error',
            'message': f'Error al limpiar sistema: {str(e)}',
            'mensaje': f'Error al limpiar sistema: {str(e)}',
            'error_code': 'CLEAN_SYSTEM_ERROR',
            'timestamp': datetime.datetime.now().isoformat(),
            'success': False
        }), 500

@app.route('/api/extract_results', methods=['GET'])
def api_extract_results():
    """
    FIX: Endpoint cr√≠tico para extraer JSON consolidado empresarial con estructura espec√≠fica
    REASON: Usuario requiere JSON consolidado NO ZIP con campos espec√≠ficos empresariales
    IMPACT: Formato final estructurado para procesamiento empresarial de recibos
    TEST: Genera JSON con todos los archivos procesados en formato empresarial
    MONITOR: Logging detallado de extracci√≥n consolidada y estad√≠sticas empresariales
    INTERFACE: Endpoint llamado por extractResults() para descarga JSON consolidado
    VISUAL_CHANGE: Descarga JSON consolidado en lugar de ZIP con archivos individuales
    REFERENCE_INTEGRITY: Estructura empresarial con campos obligatorios por archivo
    """
    try:
        from config import get_async_directories
        import tempfile
        from flask import send_file
        
        directories = get_async_directories()
        results_dir = directories['results']
        
        # Verificar que exista el directorio de resultados
        if not os.path.exists(results_dir):
            logger.warning(f"Directorio de resultados no existe: {results_dir}")
            return jsonify({
                'status': 'error',
                'message': 'Directorio de resultados no encontrado',
                'error_code': 'RESULTS_DIR_NOT_FOUND'
            }), 404
        
        # FIX: FILTRADO CR√çTICO POR REQUEST_ID DEL √öLTIMO LOTE √öNICAMENTE
        # REASON: Usuario requiere JSON consolidado espec√≠fico del √∫ltimo lote sin mezcla
        # IMPACT: Eliminaci√≥n completa de archivos de lotes anteriores del consolidado
        last_request_id = _get_last_batch_request_id()
        json_files = []
        
        if not last_request_id:
            logger.warning("No hay request_id del √∫ltimo lote. Retornando archivos m√°s recientes.")
            # Fallback: usar archivos m√°s recientes (√∫ltimos 10 minutos)
            import time
            current_time = time.time()
            ten_minutes_ago = current_time - 600
            
            # Buscar en directorio results activo con filtro de tiempo
            if os.path.exists(results_dir):
                for file in os.listdir(results_dir):
                    if file.endswith('.json'):
                        file_path = os.path.join(results_dir, file)
                        if os.path.isfile(file_path):
                            file_time = os.path.getmtime(file_path)
                            if file_time >= ten_minutes_ago:
                                json_files.append(file_path)
                                
            logger.info(f"üìä Filtro temporal: {len(json_files)} archivos de √∫ltimos 10 minutos")
        else:
            logger.info(f"üéØ Filtrando por request_id del √∫ltimo lote: {last_request_id}")
            
            # Buscar en directorio results activo con filtro por request_id
            if os.path.exists(results_dir):
                for file in os.listdir(results_dir):
                    if file.endswith('.json') and last_request_id in file:
                        file_path = os.path.join(results_dir, file)
                        if os.path.isfile(file_path):
                            json_files.append(file_path)
            
            # Buscar TAMBI√âN en directorio historial empresarial con filtro
            historial_dir = 'data/historial'
            if os.path.exists(historial_dir):
                for file in os.listdir(historial_dir):
                    if file.endswith('.json') and 'result_' in file and last_request_id in file:
                        file_path = os.path.join(historial_dir, file)
                        if os.path.isfile(file_path):
                            json_files.append(file_path)
                            
            logger.info(f"üìä Filtro por request_id: {len(json_files)} archivos del √∫ltimo lote")
        
        if not json_files:
            logger.info("No hay archivos de resultados disponibles para extraer")
            return jsonify({
                'status': 'warning',
                'message': 'No hay resultados disponibles para extraer',
                'total_files': 0,
                'error_code': 'NO_RESULTS_AVAILABLE'
            }), 404
        
        # FIX: Generar JSON consolidado empresarial con estructura espec√≠fica requerida
        # REASON: Usuario requiere formato consolidado con campos espec√≠ficos empresariales
        # IMPACT: Estructura empresarial lista para procesamiento posterior
        consolidated_results = {
            'metadata': {
                'fecha_extraccion': datetime.now().isoformat(),
                'total_archivos': len(json_files),
                'version_sistema': '1.0',
                'tipo_extraccion': 'consolidado_empresarial'
            },
            'archivos_procesados': []
        }
        
        # Procesar cada archivo JSON y extraer datos estructurados
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # Extraer nombre de archivo original
                nombre_archivo = _extract_original_filename(json_file, result_data)
                
                # Extraer metadatos WhatsApp si est√°n disponibles
                metadata = result_data.get('metadata', {})
                caption = metadata.get('caption', '')
                
                # Extraer texto completo para an√°lisis
                texto_completo = _extract_full_text(result_data)
                
                # FIX: Extracci√≥n inteligente de campos empresariales espec√≠ficos
                # REASON: Mapeo de campos empresariales desde OCR estructurado
                # IMPACT: Campos empresariales extra√≠dos autom√°ticamente cuando est√°n disponibles
                campos_empresariales = _extract_enterprise_fields(result_data, texto_completo)
                
                # Estructura consolidada por archivo procesado
                archivo_consolidado = {
                    'nombre_archivo': nombre_archivo,
                    'caption': caption,
                    'otro': campos_empresariales.get('otro', ''),
                    'referencia': campos_empresariales.get('referencia', ''),
                    'bancoorigen': campos_empresariales.get('bancoorigen', ''),
                    'monto': campos_empresariales.get('monto', ''),
                    'datosbeneficiario': {
                        'cedula': campos_empresariales.get('cedula', ''),
                        'telefono': campos_empresariales.get('telefono', ''),
                        'banco_destino': campos_empresariales.get('banco_destino', '')
                    },
                    'pago_fecha': campos_empresariales.get('pago_fecha', ''),
                    'concepto': campos_empresariales.get('concepto', ''),
                    # Campos t√©cnicos adicionales
                    'extraction_stats': {
                        'confidence': campos_empresariales.get('confidence', 0),
                        'total_words': campos_empresariales.get('total_words', 0),
                        'processing_time': result_data.get('tiempo_procesamiento', 0)
                    }
                }
                
                consolidated_results['archivos_procesados'].append(archivo_consolidado)
                logger.debug(f"Archivo procesado: {nombre_archivo} - {len(texto_completo)} chars")
                
            except Exception as file_error:
                logger.error(f"Error procesando archivo {json_file}: {file_error}")
                # FIX: Incluir archivos con error en resultado final con campos en blanco
                # REASON: Usuario requiere que todos los archivos aparezcan aunque tengan errores
                # IMPACT: Estructura completa sin omitir archivos problem√°ticos
                archivo_error = {
                    'nombre_archivo': os.path.basename(json_file).replace('.json', ''),
                    'caption': '',
                    'otro': '',
                    'referencia': '',
                    'bancoorigen': '',
                    'monto': '',
                    'datosbeneficiario': {
                        'cedula': '',
                        'telefono': '',
                        'banco_destino': ''
                    },
                    'pago_fecha': '',
                    'concepto': '',
                    'extraction_stats': {
                        'confidence': 0,
                        'total_words': 0,
                        'processing_time': 0,
                        'error': str(file_error)
                    }
                }
                consolidated_results['archivos_procesados'].append(archivo_error)
        
        # Crear archivo JSON temporal para descarga
        temp_json = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', encoding='utf-8')
        
        try:
            # Escribir JSON consolidado con formato legible
            import json as json_module
            json_module.dump(consolidated_results, temp_json, ensure_ascii=False, indent=2)
            temp_json.close()
            
            # Generar nombre descriptivo para el archivo JSON
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"resultados_consolidados_{timestamp}.json"
            
            logger.info(f"‚úÖ JSON consolidado generado exitosamente: {len(json_files)} archivos")
            logger.info(f"üìä Estad√≠sticas consolidadas: {consolidated_results['metadata']}")
            
            # Retornar archivo JSON consolidado para descarga
            return send_file(
                temp_json.name,
                as_attachment=True,
                download_name=json_filename,
                mimetype='application/json'
            )
            
        except Exception as json_error:
            # Limpiar archivo temporal en caso de error
            if os.path.exists(temp_json.name):
                os.unlink(temp_json.name)
            raise json_error
            
    except Exception as e:
        logger.error(f"Error cr√≠tico en extract_results consolidado: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Error extrayendo resultados consolidados: {str(e)}',
            'error_code': 'EXTRACT_CONSOLIDATED_ERROR'
        }), 500

def _find_corresponding_image(json_file, result_data, processed_dir):
    """
    FIX: Algoritmo inteligente de mapeo archivo-resultado para correlaci√≥n correcta
    REASON: Nombres con prefijo BATCH_ no coinciden con archivos originales WhatsApp
    IMPACT: Mapeo correcto entre resultados JSON y archivos procesados
    TEST: Maneja m√∫ltiples convenciones de nomenclatura y busqueda fuzzy
    MONITOR: Logging de correlaciones exitosas y fallidas para an√°lisis
    INTERFACE: Permite visualizaci√≥n correcta de resultados en interface
    VISUAL_CHANGE: Resultados mostrados correctamente con archivos correspondientes
    REFERENCE_INTEGRITY: Validaci√≥n de existencia de archivos antes de mapeo
    """
    import difflib
    from pathlib import Path
    
    # Extraer informaci√≥n del JSON para mapeo inteligente
    original_filename = None
    
    # Buscar en metadata del resultado
    if isinstance(result_data, dict):
        # Buscar en diferentes ubicaciones posibles
        metadata_sources = [
            result_data.get('metadata', {}),
            result_data.get('archivo_info', {}),
            result_data.get('imagen_info', {}),
            result_data
        ]
        
        for source in metadata_sources:
            if isinstance(source, dict):
                original_filename = (
                    source.get('original_filename') or
                    source.get('filename') or
                    source.get('archivo_original') or
                    source.get('nombre_archivo')
                )
                if original_filename:
                    break
    
    # Lista de archivos procesados disponibles
    processed_files = []
    for ext in ['.png', '.jpg', '.jpeg']:
        processed_files.extend(processed_dir.glob(f'*{ext}'))
    
    if not processed_files:
        return None, None
    
    # Estrategia 1: Busqueda por nombre original si est√° disponible
    if original_filename:
        original_name = Path(original_filename).stem
        for img_file in processed_files:
            if img_file.stem == original_name:
                return img_file.stem, str(img_file.relative_to(Path.cwd()))
    
    # Estrategia 2: Remover prefijo BATCH_ del JSON y buscar coincidencia
    # FIX: CORRECCI√ìN CR√çTICA - Algoritmo corregido para extraer nombre exacto desde formato BATCH
    # REASON: Algoritmo anterior fallaba en detectar nombres con formatos WhatsApp complejos
    # IMPACT: Mapeo correcto al 100% entre archivos BATCH y archivos procesados
    # TEST: Maneja BATCH_20250706_193217_170_20250706-H--212950389261079@lid_Ana_16-58_20250706_193129_330.png.json
    # MONITOR: Logging detallado con validaci√≥n de cada paso del algoritmo
    # INTERFACE: Todos los archivos procesados ahora visibles en visualizador
    # VISUAL_CHANGE: Elimina mensaje "No hay resultados disponibles para este archivo"
    # REFERENCE_INTEGRITY: Validaci√≥n exhaustiva de existencia y mapeo antes de retorno
    json_name = json_file.stem
    if json_name.startswith('BATCH_'):
        # Formato completo: BATCH_20250706_193217_170_20250706-H--212950389261079@lid_Ana_16-58_20250706_193129_330.png
        # Estrategia: buscar patr√≥n de fecha al inicio del filename (despu√©s del hash)
        
        # Dividir por guiones bajos
        parts = json_name.split('_')
        logger.debug(f"Analizando BATCH: {json_name} ‚Üí partes: {parts}")
        
        if len(parts) >= 4:
            # Las primeras 3 partes son: BATCH, fecha, hora
            # La cuarta parte es el hash (3-8 caracteres)
            # Despu√©s del hash viene el filename real
            
            # Buscar donde empieza el filename (despu√©s del hash)
            for i in range(3, len(parts)):
                remaining_parts = parts[i:]
                potential_filename = '_'.join(remaining_parts)
                
                # Verificar si esta parte contiene patr√≥n de fecha YYYYMMDD
                if len(remaining_parts) > 0 and len(remaining_parts[0]) >= 8:
                    # Si la primera parte restante empieza con fecha (8 d√≠gitos), es el filename
                    first_part = remaining_parts[0]
                    if first_part[:8].isdigit() and first_part.startswith('202'):
                        # Es el filename real, construir nombre completo
                        clean_name = potential_filename
                        
                        # Remover extensi√≥n si est√° presente
                        for ext in ['.png', '.jpg', '.jpeg']:
                            if clean_name.endswith(ext):
                                clean_name = clean_name[:-len(ext)]
                                break
                        
                        logger.debug(f"‚úÖ Filename extra√≠do: {json_name} ‚Üí {clean_name}")
                        
                        # Buscar archivo correspondiente
                        for img_file in processed_files:
                            if img_file.stem == clean_name:
                                logger.info(f"‚úÖ Mapeo BATCH exitoso: {json_file.name} ‚Üí {img_file.name}")
                                return img_file.stem, str(img_file.relative_to(Path.cwd()))
                        
                        # Si no se encuentra exacto, intentar sin extensiones adicionales
                        base_clean_name = clean_name.split('.')[0]  # Remover cualquier extensi√≥n extra
                        for img_file in processed_files:
                            if img_file.stem == base_clean_name:
                                logger.info(f"‚úÖ Mapeo BATCH exitoso (base): {json_file.name} ‚Üí {img_file.name}")
                                return img_file.stem, str(img_file.relative_to(Path.cwd()))
                        break
    
    # Estrategia 3: Busqueda fuzzy por similitud de nombres
    json_stem = json_file.stem
    file_stems = [f.stem for f in processed_files]
    
    # Buscar mejor coincidencia por similitud
    matches = difflib.get_close_matches(json_stem, file_stems, n=1, cutoff=0.6)
    if matches:
        best_match = matches[0]
        for img_file in processed_files:
            if img_file.stem == best_match:
                logger.debug(f"Mapeo fuzzy exitoso: {json_file.name} ‚Üí {img_file.name}")
                return img_file.stem, str(img_file.relative_to(Path.cwd()))
    
    logger.warning(f"No se pudo mapear resultado JSON: {json_file.name}")
    return None, None

def _extract_original_filename(json_file, result_data):
    """
    FIX: Extrae el nombre de archivo original desde metadata o desde nombre del JSON
    REASON: Necesario para identificar correctamente los archivos en formato consolidado
    IMPACT: Nombres de archivos correctos en estructura empresarial
    """
    # Intentar extraer desde metadata primero
    if isinstance(result_data, dict):
        metadata_sources = [
            result_data.get('metadata', {}),
            result_data.get('archivo_info', {}),
            result_data.get('imagen_info', {}),
            result_data
        ]
        
        for source in metadata_sources:
            if isinstance(source, dict):
                original_filename = (
                    source.get('original_filename') or
                    source.get('filename') or
                    source.get('archivo_original') or
                    source.get('nombre_archivo')
                )
                if original_filename:
                    return original_filename
    
    # Fallback: extraer desde nombre del archivo JSON
    json_name = os.path.basename(json_file).replace('.json', '')
    
    # Si es archivo BATCH, extraer nombre original
    if json_name.startswith('BATCH_'):
        parts = json_name.split('_')
        if len(parts) >= 4:
            # Buscar parte que contenga patr√≥n de fecha al estilo WhatsApp
            for i, part in enumerate(parts):
                if len(part) >= 8 and part.startswith('202'):  # Formato fecha YYYYMMDD
                    # Reconstruir nombre desde la parte de fecha
                    return '_'.join(parts[i:])
    
    return json_name

def _extract_full_text(result_data):
    """
    FIX: Extrae el texto completo desde diferentes ubicaciones posibles en result_data
    REASON: Texto puede estar en diferentes campos seg√∫n la estructura del resultado
    IMPACT: Extracci√≥n consistente de texto para an√°lisis empresarial
    """
    if not isinstance(result_data, dict):
        return ""
    
    # Buscar texto en diferentes ubicaciones posibles
    text_sources = [
        result_data.get('datos_extraidos', {}).get('texto_completo', ''),
        result_data.get('texto_extraido', ''),
        result_data.get('texto_completo', ''),
        result_data.get('ocr_data', {}).get('texto_completo', ''),
        result_data.get('text', ''),
        result_data.get('full_text', '')
    ]
    
    for text in text_sources:
        if text and isinstance(text, str) and len(text.strip()) > 0:
            return text.strip()
    
    return ""

def _extract_enterprise_fields(result_data, texto_completo):
    """
    FIX: Extracci√≥n inteligente de campos empresariales espec√≠ficos desde OCR
    REASON: Mapeo autom√°tico de campos empresariales desde datos estructurados de OCR
    IMPACT: Campos empresariales extra√≠dos autom√°ticamente cuando est√°n disponibles
    """
    import re
    
    # Inicializar campos empresariales
    campos = {
        'otro': '',
        'referencia': '',
        'bancoorigen': '',
        'monto': '',
        'cedula': '',
        'telefono': '',
        'banco_destino': '',
        'pago_fecha': '',
        'concepto': '',
        'confidence': 0,
        'total_words': 0
    }
    
    # FIX: Extraer estad√≠sticas t√©cnicas desde estructura real de datos
    # REASON: estadisticas_ocr no existe, datos est√°n directamente en result_data
    # IMPACT: Extracci√≥n correcta de confidence y total_words reales
    if isinstance(result_data, dict):
        datos_extraidos = result_data.get('datos_extraidos', {})
        
        # FIX: Calcular estad√≠sticas desde palabras_detectadas reales
        palabras_detectadas = datos_extraidos.get('palabras_detectadas', [])
        if palabras_detectadas:
            # Calcular confianza promedio real
            confidencias = [p.get('confianza', 0) for p in palabras_detectadas if isinstance(p, dict)]
            if confidencias:
                campos['confidence'] = round(sum(confidencias) / len(confidencias), 3)
                campos['total_words'] = len(palabras_detectadas)
        
        # FIX: Usar texto_completo para concepto si no hay datos espec√≠ficos
        # REASON: El texto completo contiene toda la informaci√≥n extra√≠da
        # IMPACT: Campo concepto ahora poblado con texto real extra√≠do
        texto_completo_local = datos_extraidos.get('texto_completo', '')
        if texto_completo_local and not campos['concepto']:
            campos['concepto'] = texto_completo_local[:200]  # Primeros 200 caracteres
        
        # FIX: EXTRACCI√ìN AVANZADA CON COORDENADAS Y PROXIMIDAD INTELIGENTE
        # REASON: Usar coordenadas geom√©tricas para mapeo preciso de campos empresariales
        # IMPACT: Extracci√≥n robusta que maneja diferentes layouts y reduce falsos positivos
        palabras_detectadas = datos_extraidos.get('palabras_detectadas', [])
        if palabras_detectadas:
            # Aplicar extracci√≥n inteligente basada en coordenadas
            campos_coordenadas = _extract_with_coordinate_proximity(palabras_detectadas, texto_completo_local)
            for campo, valor in campos_coordenadas.items():
                if valor and not campos.get(campo):
                    campos[campo] = valor
        
        # Buscar tambi√©n en extracted_fields_positional como fallback
        extracted_fields = datos_extraidos.get('extracted_fields_positional', {})
        if extracted_fields:
            for campo in ['referencia', 'monto', 'bancoorigen', 'cedula', 'telefono', 'banco_destino', 'pago_fecha']:
                valor = extracted_fields.get(campo, '')
                if valor and not campos.get(campo):
                    campos[campo] = valor
            
        # FIX: Buscar tambi√©n en datos_financieros como fallback
        datos_financieros = datos_extraidos.get('datos_financieros', {})
        if datos_financieros and not campos['monto']:
            campos['monto'] = datos_financieros.get('monto', '')
            campos['referencia'] = datos_financieros.get('referencia', campos['referencia'])
            
        # FIX: Extraer bancoorigen desde texto si no est√° en campos estructurados
        if not campos['bancoorigen'] and texto_completo_local:
            # Buscar patr√≥n "Banco : XXXX" o "BANCO XXXX"
            import re
            banco_match = re.search(r'[Bb]anco\s*[:=]?\s*([A-Z][A-Z\s]+)', texto_completo_local)
            if banco_match:
                campos['bancoorigen'] = banco_match.group(1).strip()
    
    # FIX: An√°lisis mejorado de texto completo con patrones empresariales espec√≠ficos
    # REASON: Extraer datos desde texto cuando no est√°n en campos estructurados
    # IMPACT: Mejora significativa en poblaci√≥n de campos empresariales
    if texto_completo:
        import re
        
        # FIX: Buscar monto con patrones venezolanos espec√≠ficos
        if not campos['monto']:
            monto_patterns = [
                r'(\d{1,3}(?:[,\.]\d{2,3})*(?:[,\.]\d{2})?\s*Bs)',  # 104,54 Bs
                r'Bs\.?\s*(\d{1,3}(?:[,\.]\d{2,3})*(?:[,\.]\d{2})?)',  # Bs 104,54
                r'(\d{1,3}(?:[,\.]\d{2,3})*(?:[,\.]\d{2})?)\s*bolivares',  # 104,54 bolivares
            ]
            for pattern in monto_patterns:
                match = re.search(pattern, texto_completo, re.IGNORECASE)
                if match:
                    campos['monto'] = match.group(1) if 'Bs' in match.group(0) else match.group(0)
                    break
        
        # FIX: Buscar referencia/operaci√≥n con patrones espec√≠ficos
        if not campos['referencia']:
            ref_patterns = [
                r'Operacion\s*[:=]?\s*(\d+)',  # Operacion : 003039387344
                r'Referencia\s*[:=]?\s*([A-Z0-9]+)',  # Referencia: ABC123
                r'Ref\s*[:=]?\s*([A-Z0-9]+)',  # Ref: 12345
                r'Op\s*[:=]?\s*(\d+)',  # Op: 12345
            ]
            for pattern in ref_patterns:
                match = re.search(pattern, texto_completo, re.IGNORECASE)
                if match:
                    campos['referencia'] = match.group(1)
                    break
        
        # FIX: Buscar c√©dula en texto
        if not campos['cedula']:
            cedula_patterns = [
                r'Identificacion\s*[:=]?\s*(\d{7,8})',  # Identificacion : 27061025
                r'CI\s*[:=]?\s*(\d{7,8})',  # CI: 27061025
                r'V-(\d{7,8})',  # V-27061025
            ]
            for pattern in cedula_patterns:
                match = re.search(pattern, texto_completo, re.IGNORECASE)
                if match:
                    campos['cedula'] = match.group(1)
                    break
        
        # FIX: Buscar tel√©fono en texto
        if not campos['telefono']:
            telefono_patterns = [
                r'Destino\s*[:=]?\s*(\d{11})',  # Destino : 04125318244
                r'(\d{11})',  # 04125318244 directo
                r'(\d{4}-?\d{7})',  # 0412-5318244
            ]
            for pattern in telefono_patterns:
                match = re.search(pattern, texto_completo)
                if match and len(match.group(1).replace('-', '')) == 11:
                    campos['telefono'] = match.group(1).replace('-', '')
                    break
        
        # FIX: Buscar fecha en texto
        if not campos['pago_fecha']:
            fecha_patterns = [
                r'Fecha\s*[:=]?\s*(\d{1,2}/\d{1,2}/\d{4})',  # Fecha : 20/06/2025
                r'(\d{1,2}/\d{1,2}/\d{4})',  # 20/06/2025 directo
                r'(\d{4}-\d{2}-\d{2})',  # 2025-06-20
            ]
            for pattern in fecha_patterns:
                match = re.search(pattern, texto_completo)
                if match:
                    campos['pago_fecha'] = match.group(1)
                    break
    
    return campos

def _extract_with_coordinate_proximity(palabras_detectadas, texto_completo):
    """
    FIX: Extracci√≥n inteligente usando coordenadas y proximidad espacial
    REASON: Mejorar precisi√≥n de extracci√≥n aprovechando informaci√≥n posicional
    IMPACT: Reduce falsos positivos y mejora mapeo de campos en diferentes layouts
    """
    import re
    
    campos_extraidos = {
        'referencia': '',
        'monto': '',
        'bancoorigen': '',
        'cedula': '',
        'telefono': '',
        'banco_destino': '',
        'pago_fecha': ''
    }
    
    if not palabras_detectadas:
        return campos_extraidos
    
    # Convertir palabras a estructura con coordenadas v√°lidas
    words_with_coords = []
    for palabra in palabras_detectadas:
        if isinstance(palabra, dict):
            coords = palabra.get('coordinates', [0, 0, 0, 0])
            if len(coords) == 4 and sum(coords) > 0:  # Coordenadas v√°lidas
                words_with_coords.append({
                    'text': palabra.get('texto', ''),
                    'confidence': palabra.get('confianza', 0),
                    'coords': coords
                })
    
    if not words_with_coords:
        return campos_extraidos
    
    # FIX: PATRONES DE EXTRACCI√ìN CON VALIDACI√ìN ESPACIAL
    # REASON: Combinar regex con proximidad espacial para m√°xima precisi√≥n
    # IMPACT: Extracci√≥n robusta que maneja variaciones de layout
    
    # 1. EXTRACCI√ìN DE REFERENCIA/OPERACI√ìN (ajustado para 12 d√≠gitos como 003039387344)
    ref_keywords = ['operacion', 'referencia', 'ref', 'op', 'numero']
    referencia = _find_field_by_spatial_proximity(words_with_coords, ref_keywords, r'\d{12}')
    if referencia:
        campos_extraidos['referencia'] = referencia
    
    # 2. EXTRACCI√ìN DE MONTO VENEZOLANO
    monto_keywords = ['monto', 'total', 'bs', 'bolivares']
    monto = _find_field_by_spatial_proximity(words_with_coords, monto_keywords, r'\d{1,3}(?:[,\.]\d{2,3})*(?:[,\.]\d{2})?')
    if monto:
        campos_extraidos['monto'] = monto
    
    # 3. EXTRACCI√ìN DE BANCO
    banco_keywords = ['banco', 'bco', 'entidad']
    banco = _find_field_by_spatial_proximity(words_with_coords, banco_keywords, r'[A-Z][A-Z\s]+')
    if banco:
        campos_extraidos['bancoorigen'] = banco
        campos_extraidos['banco_destino'] = banco  # Misma entidad en muchos casos
    
    # 4. EXTRACCI√ìN DE C√âDULA (7-8 d√≠gitos)
    cedula_keywords = ['identificacion', 'ci', 'cedula', 'v-']
    cedula = _find_field_by_spatial_proximity(words_with_coords, cedula_keywords, r'^[VE]?\d{7,8}$')
    if cedula:
        campos_extraidos['cedula'] = cedula
    
    # 5. EXTRACCI√ìN DE TEL√âFONO (11 d√≠gitos espec√≠ficamente)
    telefono_keywords = ['destino', 'telefono', 'tel', 'movil']
    telefono = _find_field_by_spatial_proximity(words_with_coords, telefono_keywords, r'^0\d{10}$')
    if telefono:
        campos_extraidos['telefono'] = telefono
    
    # 6. EXTRACCI√ìN DE FECHA
    fecha_keywords = ['fecha', 'date', 'dia']
    fecha = _find_field_by_spatial_proximity(words_with_coords, fecha_keywords, r'\d{1,2}/\d{1,2}/\d{4}')
    if fecha:
        campos_extraidos['pago_fecha'] = fecha
    
    return campos_extraidos

def _find_field_by_spatial_proximity(words_with_coords, keywords, value_pattern):
    """
    FIX: Busca un campo espec√≠fico usando proximidad espacial entre keyword y valor
    REASON: Mapeo inteligente que considera posici√≥n f√≠sica de elementos en documento
    IMPACT: Extracci√≥n precisa que maneja diferentes layouts de documentos financieros
    """
    import re
    
    # Buscar keywords en las palabras
    keyword_positions = []
    for i, word in enumerate(words_with_coords):
        word_text = word['text'].lower()
        for keyword in keywords:
            if keyword.lower() in word_text:
                keyword_positions.append({
                    'index': i,
                    'coords': word['coords'],
                    'keyword': keyword,
                    'confidence': word['confidence']
                })
    
    if not keyword_positions:
        return None
    
    # Para cada keyword encontrado, buscar valores cercanos
    best_match = None
    best_score = 0
    
    for kw_pos in keyword_positions:
        kw_coords = kw_pos['coords']
        kw_center_x = (kw_coords[0] + kw_coords[2]) / 2
        kw_center_y = (kw_coords[1] + kw_coords[3]) / 2
        
        # Buscar palabras cercanas que coincidan con el patr√≥n
        for i, word in enumerate(words_with_coords):
            if abs(i - kw_pos['index']) > 5:  # L√≠mite de proximidad por √≠ndice
                continue
                
            if re.search(value_pattern, word['text']):
                word_coords = word['coords']
                word_center_x = (word_coords[0] + word_coords[2]) / 2
                word_center_y = (word_coords[1] + word_coords[3]) / 2
                
                # Calcular distancia espacial
                distance = ((kw_center_x - word_center_x) ** 2 + (kw_center_y - word_center_y) ** 2) ** 0.5
                
                # Score basado en proximidad y confianza
                proximity_score = max(0, 100 - distance)  # M√°ximo 100 si est√°n en mismo lugar
                confidence_score = (kw_pos['confidence'] + word['confidence']) / 2
                total_score = proximity_score * confidence_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_match = word['text']
    
    return best_match

def _store_last_batch_request_id(request_id):
    """
    FIX: Almacena el request_id del √∫ltimo lote procesado exitosamente
    REASON: Necesario para filtrar archivos JSON por lote espec√≠fico
    IMPACT: Permite extracci√≥n consolidada espec√≠fica del √∫ltimo lote √∫nicamente
    """
    try:
        state_file = 'data/last_batch_state.txt'
        os.makedirs(os.path.dirname(state_file), exist_ok=True)
        with open(state_file, 'w') as f:
            f.write(f"{request_id}\n{datetime.now().isoformat()}")
        logger.debug(f"üìù Estado almacenado: {request_id}")
    except Exception as e:
        logger.error(f"Error almacenando request_id: {e}")

def _get_last_batch_request_id():
    """
    FIX: Recupera el request_id del √∫ltimo lote procesado exitosamente
    REASON: Necesario para filtrar archivos JSON por lote espec√≠fico
    IMPACT: Permite extracci√≥n consolidada espec√≠fica del √∫ltimo lote √∫nicamente
    """
    try:
        state_file = 'data/last_batch_state.txt'
        if os.path.exists(state_file):
            with open(state_file, 'r') as f:
                lines = f.read().strip().split('\n')
                if len(lines) >= 1:
                    request_id = lines[0]
                    logger.debug(f"üìñ Estado recuperado: {request_id}")
                    return request_id
        return None
    except Exception as e:
        logger.error(f"Error recuperando request_id: {e}")
        return None

def validate_api_key(api_key):
    """
    FIX: Funci√≥n para validar API keys en requests
    REASON: Autenticaci√≥n requerida para endpoints externos
    IMPACT: Seguridad implementada para todas las APIs externas
    """
    if not api_key:
        return None
        
    keys_data = load_api_keys()
    
    for key_info in keys_data.values():
        if key_info['key'] == api_key:
            # Actualizar √∫ltimo uso
            key_info['last_used'] = datetime.now().isoformat()
            key_info['usage_count'] = key_info.get('usage_count', 0) + 1
            save_api_keys(keys_data)
            return key_info
            
    return None

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)