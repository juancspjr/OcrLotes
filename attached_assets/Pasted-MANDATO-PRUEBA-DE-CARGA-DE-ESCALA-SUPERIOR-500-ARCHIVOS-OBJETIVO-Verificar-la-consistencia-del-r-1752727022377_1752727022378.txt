MANDATO: PRUEBA DE CARGA DE ESCALA SUPERIOR (500 ARCHIVOS)
OBJETIVO: Verificar la consistencia del rendimiento y la estabilidad del Sistema OCR Empresarial con un volumen de 500 archivos, para revelar posibles limitaciones en el uso de memoria, CPU sostenida, y I/O, y confirmar que el rendimiento se mantiene lineal o cercano a lineal.

CANTIDAD DE ARCHIVOS: 500 archivos representativos (similares a los usados en la prueba de 50 archivos).

1. FASES DEL MANDATO - ACCIONES A EJECUTAR:
FASE 1: PREPARACIÓN DEL ENTORNO Y DATOS
1.1 Preparar Datos de Prueba:

Asegúrate de tener 500 archivos de imagen (PNG, JPG, JPEG) disponibles. Es crucial que estos archivos sean representativos de los datos reales que el sistema procesará en producción (similares en tamaño, complejidad, tipo de contenido).

Recomendación: Coloca estos archivos en un directorio temporal para fácil acceso y gestión durante la prueba.

1.2 Limpiar Entorno Previo:

Elimina cualquier archivo de resultados o logs de pruebas anteriores para asegurar que las métricas recolectadas sean solo de esta prueba.

Comando Sugerido: rm -rf data/results/* data/historial/* (¡Úsalo con precaución, solo si estás seguro de que no necesitas los datos anteriores!).

1.3 Asegurar Recursos:

Verifica que el sistema host tenga suficientes recursos de CPU, RAM y espacio en disco disponibles antes de iniciar la prueba.

FASE 2: EJECUCIÓN DE LA PRUEBA DE CARGA (500 ARCHIVOS)
Objetivo: Iniciar el procesamiento del lote de 500 archivos y monitorear el sistema en tiempo real.

Acciones:

2.1 Iniciar la Aplicación: Asegúrate de que tu aplicación Flask/Gunicorn esté corriendo en el modo optimizado.

Comando Sugerido: gunicorn --workers 4 --threads 2 --bind 0.0.0.0:5000 main:app (Ajusta workers y threads según tus núcleos de CPU y experimentación previa).

2.2 Iniciar el Monitoreo en Tiempo Real: Abre terminales separadas para observar el uso de recursos mientras la prueba está en curso.

Para CPU/Memoria (General): top o htop (recomendado si está disponible).

Para I/O de Disco: iostat -x 1 (si está disponible, observa r/s, w/s, %util).

Para Logs: tail -f logs/app.log (o la ruta de tu archivo de log principal para ver errores o warnings).

2.3 Lanzar la Carga de 500 Archivos:

Utiliza un script (Python, Bash) o una herramienta como curl en un bucle para subir y procesar los 500 archivos de forma consecutiva o concurrente, dependiendo de cómo simules el flujo de trabajo real.

Ejemplo simplificado de un bucle en Bash (adaptar a tu endpoint de subida):

Bash

START_TIME=$(date +%s.%N)
for i in {1..500}; do
    # Asumiendo un archivo de imagen genérico para la prueba: test_image.png
    # Adapta la ruta al archivo y el endpoint de tu API
    curl -X POST -H "Content-Type: multipart/form-data" \
         -F "file=@/path/to/your/test_images/image_$i.png" \
         http://localhost:5000/api/ocr/upload > /dev/null & # '&' para concurrencia
    # Puedes ajustar el sleep si quieres controlar la concurrencia/intervalo
    # sleep 0.01 
done
wait # Esperar a que todos los procesos en background terminen
END_TIME=$(date +%s.%N)
DURATION=$(echo "$END_TIME - $START_TIME" | bc)
echo "Tiempo Total del Lote (Iniciación de subidas): $DURATION segundos"
(Asegúrate de que tus archivos de prueba existan y sean nombrados consecuentemente, o adapta el bucle para iterar sobre los nombres de archivo reales).

FASE 3: RECOPILACIÓN EXHAUSTIVA Y ANÁLISIS DE MÉTRICAS
Objetivo: Consolidar y analizar los datos de rendimiento para obtener una imagen precisa del comportamiento del sistema bajo carga.

Acciones:

3.1 Recopilación de Tiempos de Procesamiento Individuales:

Una vez que todos los archivos se hayan procesado (verifica que el contador de la cola esté en cero o que todos los archivos resultantes estén en data/results/ o data/historial/), recopila los processing_time de cada JSON de resultado.

Comando Sugerido (para JSONs de resultados individuales):

Bash

echo "--- ANÁLISIS DE TIEMPOS DE PROCESAMIENTO INDIVIDUALES ---"
find data/results/ -name "*.json" -print0 | xargs -0 -I {} \
    sh -c 'cat "{}" | python3 -c "import json,sys; data=json.load(sys.stdin); pt=data.get(\"extraction_stats\",{}).get(\"processing_time\", \"N/A\"); if pt == 0: pt=\"0 (instant)\"; print(f\"{pt}\")" 2>/dev/null' > processing_times_500.txt

# Calcular estadísticas
echo "--- ESTADÍSTICAS GLOBALES DE TIEMPO POR ARCHIVO ---"
awk '{ sum += $1; count++ } END { print "Suma:", sum, "Count:", count, "Avg:", sum/count }' processing_times_500.txt
sort -n processing_times_500.txt | awk 'BEGIN {c=0;} {a[c++]=$0;} END {print "Mediana:", (a[int(c/2)] + a[int((c-1)/2)])/2, "Min:", a[0], "Max:", a[c-1];}'
(Nota: El processing_time en algunos de tus JSONs de ejemplo era 0. Asegúrate de que tu sistema esté registrando tiempos precisos, o considera usar el tiempo_total_del_lote / archivos_procesados como métrica principal si los individuales son cero.)

3.2 Cálculo de Métricas Clave:

Tiempo Total del Lote: Es el DURATION obtenido del script de la Fase 2.

Archivos Procesados: Cuenta los JSONs generados en data/results/ o data/historial/.

Comando Sugerido: find data/results/ data/historial/ -name "*.json" | wc -l

Throughput (archivos/segundo): Archivos Procesados / Tiempo Total del Lote.

Tasa de Éxito: Si el número de archivos procesados es igual al número de archivos enviados (500), la tasa de éxito es 100%. Si no, investiga los errores en los logs.

3.3 Análisis de Uso de Recursos (Post-Test):

Revisa los registros de top/htop y iostat que tomaste durante la prueba.

Identifica: picos máximos de CPU y RAM, uso promedio, si el disco fue un cuello de botella (%util alto), y si hubo alguna inestabilidad.

Comando Sugerido para uso de memoria y CPU (resumen): free -h y grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {print "CPU Usage:", usage "%"}' (corre al final para ver el estado)

3.4 Revisión de Logs del Sistema:

Examina cuidadosamente los logs de la aplicación (logs/app.log o similar) y los logs del servidor (si tienes acceso a ellos, ej., logs de Gunicorn o Nginx/Apache) en busca de errores, advertencias, o trazas de excepción que pudieran indicar problemas subyacentes bajo carga.

3.5 Identificación de Outliers:

Si el cálculo de processing_time individual funciona, identifica los archivos con los tiempos más altos. Investiga si hay patrones (ej., ciertos tipos de imágenes, tamaños específicos).

FASE 4: EVALUACIÓN Y CONCLUSIONES
Objetivo: Interpretar los datos recopilados y sacar conclusiones sobre el rendimiento y la escalabilidad del sistema.

Acciones:

4.1 Comparación de Rendimiento:

Compara el Throughput y el Tiempo Promedio por Archivo de la prueba de 500 archivos con los resultados de la prueba de 50 archivos.

¿Se mantiene la linealidad? Si el tiempo promedio por archivo sigue siendo bajo y el throughput es proporcionalmente alto, el sistema escala bien.

¿Hay una degradación significativa? Si el tiempo promedio o el throughput bajan drásticamente, indica un cuello de botella.

4.2 Evaluación de Estabilidad:

¿Hubo errores durante el procesamiento?

¿El uso de memoria o CPU fue insostenible o causó ralentizaciones críticas?

¿La actividad de I/O fue excesiva?

4.3 Identificación de Limitaciones (Si las hay):

Si se detectaron limitaciones, documéntalas claramente (ej., "La CPU alcanzó el 100% de uso durante X segundos", "Se observó un aumento de la latencia de disco", "El consumo de RAM subió a X GB y no se liberó").

4.4 Informe de Resultados:

Prepara un informe conciso con todas las métricas clave, las observaciones, las conclusiones y, si es necesario, las recomendaciones para futuras optimizaciones o ajustes de infraestructura.

